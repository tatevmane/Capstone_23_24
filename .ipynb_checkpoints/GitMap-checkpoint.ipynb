{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6950ae73-708e-41cd-bd83-63dab64d2d4a",
   "metadata": {},
   "source": [
    "## INFO: Git Map\n",
    "### Spring 24\n",
    "---\n",
    "<p><mark>.py files</mark></p>\n",
    "<p><b>extraction_run</b></p>\n",
    "Purpose: Takes in the source path (the path to the folder of PDFs) and the destination path (the path to the folder of TXTs) and:\n",
    "<ol>\n",
    "    <li>creates a df with information about each PDF file in the folder (index: file name, elements: source path, destination path, [FUTURE FEATURES: state, district, ...]);</li>\n",
    "    <li>extracts the narrative for each pdf and saves each extraction as a TXT file, then saves the TXT file at the destination path.</li>\n",
    "</ol>\n",
    "\n",
    "---\n",
    "<p><mark>.ipynb files</mark></p>\n",
    "<p><b>model-v1</b></p>\n",
    "Purpose: Using a random folder of court cases on my local drive, attempt at setting up the initial inputs (TFIDFVec.) for a logistic regression and SVM model. This notebook does the following:\n",
    "<ol>\n",
    "    <li>flushes out how to create a CORPUS from the folder on a local drive;</li>\n",
    "    <li>demonstrates how to use TFIDFVectorizer;</li>\n",
    "    <li>holds the skeleton code for the logistic regression and SVM models.</li>\n",
    "</ol>\n",
    "\n",
    "<p><b>model-v2</b></p>\n",
    "\n",
    "<p><b>NLP-pt1</b></p>\n",
    "Purpose: Using one court case document, attempt tokenization and bare basics of N-gram analysis on the document. This notebook does the following:\n",
    "<ol>\n",
    "    <li>uses NLTK to tokenize the document;</li>\n",
    "    <li>creates a dataframe TOKEN with the sentence number of token number as indices and the token str and term str (which is the token str normalized) to use in N-gram model;</li>\n",
    "    <li>creates a dataframe df1 that adds the POS tag;</li>\n",
    "    <li>demonstrates skeleton code for finding, labeling, and excluding stopwords using NLTK stopwords;</li>\n",
    "    <li>makes a function is create N-grams from a single document;</li>\n",
    "    <li>demonstrates how N-gram function works.</li>\n",
    "</ol>\n",
    "\n",
    "<p><b>NLP-pt2</b></p>\n",
    "Purpose: Using the path to the folder with a collection of .txt files (all court cases), attempt using CountVectorizer. This notebook does the following:\n",
    "<ol>\n",
    "    <li>creates a dataframe narratives that holds the narrative for each document to use with CountVectorizer;</li>\n",
    "    <li>transforms the text data into a vector using CountVectorizer to demonstrate how it works;</li>\n",
    "    <li>also creates a dataframe df that holds the sentences from one document and uses it with CountVec.</li>\n",
    "</ol>\n",
    "\n",
    "<p><b>NLP-pt3</b></p>\n",
    "Purpose: This was an UNFINISHED attempt at automating CountVectorizer. This notebook does the following:\n",
    "<ol>\n",
    "    <li>N/A</li>\n",
    "</ol>\n",
    "\n",
    "<p><b>trial_run_test</b></p>\n",
    "Purpose: Using a folder of pdfs and the source and a folder of txts as the destination, check to make sure the <mark>extraction_run.py</mark> .create_info() was running correctly. Since at this time, there were already txt documents in the destination path, there was no attempt at testing the .process_pdfs(). This notebook does the following:\n",
    "<ol>\n",
    "    <li>demonstrates how to import Extraction and initiates the source_path and destination_path;</li>\n",
    "    <li>calls .create_info() and shows the output - a dataframe INFO that has the pdf_path and txt_path for each document.</li>\n",
    "</ol>\n",
    "\n",
    "<p><b>trial_run2</b></p>\n",
    "Purpose: Using a folder of pdfs and the source and a folder of txts as the destination, check to make sure the <mark>extraction_run.py</mark> running correctly. Tests all functions EXCEPT for .add_labels(). This notebook does the following:\n",
    "<ol>\n",
    "    <li>demonstrates how to import Extraction and initiates the source_path and destination_path;</li>\n",
    "    <li>tests .get_narrative(), .save_narrative_to_txt(), flags_decomposer(), and .process_pdfs() by calling .process_pdfs();</li>\n",
    "    <li>tests .create_info() and shows the output - a dataframe INFO that has the pdf_path and txt_path for each document.</li>\n",
    "</ol>\n",
    "\n",
    "<p><b>trial_run3</b></p>\n",
    "Purpose: Using a folder of pdfs and the source and a folder of txts as the destination, check to make sure the <mark>extraction_run.py</mark> .add_labels() and <mark>model_run.py</mark> .make_corpus() was running correctly. This notebook does the following:\n",
    "<ol>\n",
    "    <li>demonstrates how to import Extraction and initiates the source_path and destination_path;</li>\n",
    "    <li>tests .create_info() to create a dataframe INFO that has the pdf_path and txt_path for each document;</li>\n",
    "    <li>imports another dataframe labels that holds the CaseName and Sex_Trafficking, applies cleaning so that the index matches the format of the INFO dataframe, and changes the column name to label;</li>\n",
    "    <li>tests .add_labels(), saving the output as a dataframe INFO that has the pdf_path, txt_path, and binary label for each document;</li>\n",
    "    <li>demonstrates how to import Model and initiates dataframe;</li>\n",
    "    <li>tests .make_corpus() and shows the output - a dataframe with document name, sentence number, and token number as the indices, and token str, term str, and pos tag as the columns.</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
