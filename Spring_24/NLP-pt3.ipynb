{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84af1e5c-f96e-43a4-98ef-c4a148f27a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d77f659-13bc-4d3f-b6f1-f4bdb3a769a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFY THIS\n",
    "# get path to your folder that holds the txt files\n",
    "source_files = \"C:/Users/jacqu/Downloads/Court Case PDFs/Court Case TXTs\"\n",
    "# outputs a list of all the txt files in the folder\n",
    "source_file_list = sorted(glob(f\"{source_files}/*.txt\"))\n",
    "\n",
    "# creates a list of tuples with an elememt for the source path and\n",
    "# for the file title\n",
    "file_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    # split might be different, recommend checking with INFO.sample() or .head()\n",
    "    file_title = source_file_path.split('\\\\')[-1].split(\".txt\")[0]\n",
    "    file_data.append((source_file_path, file_title))\n",
    "\n",
    "# creating df with the file title as the index and source path as a col\n",
    "INFO = pd.DataFrame(file_data, columns=['txt_path','file_title'])\\\n",
    "    .set_index('file_title').sort_index()\n",
    "# attempt at dropping any duplicate files with same file name\n",
    "# this only works if same file has the SAME NAME\n",
    "# See Notes below\n",
    "INFO = INFO[~INFO.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102da13-8661-4441-b300-9b02746191d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document\n",
    "for doc_idx, txt_path in enumerate(INFO['txt_path']):\n",
    "    # open up the doc\n",
    "    my_file = open(INFO.txt_path[0], \"r\") \n",
    "    # read the doc line by line\n",
    "    narrative_lines = my_file.read()\n",
    "    # make a df with each row rep. a sentence\n",
    "    sent_df = pd.DataFrame({'sent_str':nltk.sent_tokenize(narrative_lines)})\n",
    "    sent_df.sent_str = sent_df.sent_str.str.strip()\n",
    "    sent_df.index.name = \"sent_num\"\n",
    "\n",
    "    # creating the model\n",
    "    model = CountVectorizer(ngram_range = (2, 2), max_features = 100, stop_words='english')\n",
    "    # apply the model to the sentence df\n",
    "    # goal: get bigrams for one document \n",
    "    matrix = model.fit_transform(sent_df.sent_str).toarray()\n",
    "    feature_names = model.get_feature_names_out()\n",
    "    df_output = pd.DataFrame(data = matrix, columns = feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
